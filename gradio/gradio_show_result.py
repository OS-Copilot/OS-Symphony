from typing import Literal
import gradio as gr
import os
import json
from pathlib import Path
from PIL import Image, ImageDraw, ImageFont
import random
import shutil
import matplotlib.pyplot as plt
from collections import Counter
import re
import numpy as np

# ==============================================================================
# Gradio åº”ç”¨æ ¸å¿ƒé€»è¾‘
# ==============================================================================

MAX_BUTTONS = 100 # é¢„å…ˆå®šä¹‰UIä¸­æ”¯æŒçš„æœ€å¤§æŒ‰é’®æ•°é‡ï¼ˆé€‚ç”¨äºdomainå’Œtaskï¼‰

def get_domains(root_dir):
    """è·å–æ ¹ç›®å½•ä¸‹çš„æ‰€æœ‰domainç›®å½•"""
    if not os.path.isdir(root_dir):
        return []
    return [d for d in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, d))]

def _get_result_status(result_file):
    """
    ä¸€ä¸ªå†…éƒ¨è¾…åŠ©å‡½æ•°ï¼Œç”¨äºè¯»å–result.txtå¹¶è¿”å›ä¸€ä¸ªçŠ¶æ€ç ã€‚
    è¿”å›: 1 (æˆåŠŸ), 0 (å¤±è´¥), -1 (æœªçŸ¥)
    """
    if not result_file.exists():
        return -1
    try:
        result_num = float(result_file.read_text().strip())
        if result_num > 0:
            return 1
        elif result_num == 0:
            return 0
    except (ValueError, TypeError):
        return -1
    return -1

def _read_result_value(result_file):
    """è¯»å–å…·ä½“çš„resultæ•°å€¼ï¼Œç”¨äºæ˜¾ç¤º"""
    if not result_file.exists():
        return "æœªçŸ¥"
    try:
        return result_file.read_text().strip()
    except:
        return "Error"

def _get_best_task_result(root_dir, domain, task_name, merge_dirs):
    """
    æ ¸å¿ƒé€»è¾‘ï¼šåœ¨ Base è·¯å¾„å’Œæ‰€æœ‰ Merge è·¯å¾„ä¸­ï¼Œä¸ºæŒ‡å®šä»»åŠ¡å¯»æ‰¾æœ€ä¼˜ç»“æœã€‚
    è¿”å›: (best_status, best_root, best_result_str)
    """
    candidate_roots = [root_dir]
    if merge_dirs:
        candidate_roots.extend(merge_dirs)

    best_status = -2 
    best_root = root_dir
    best_result_str = "0.0"

    for r_dir in candidate_roots:
        current_task_path = Path(r_dir) / domain / task_name
        if not current_task_path.exists():
            continue
        
        res_file = current_task_path / "result.txt"
        status = _get_result_status(res_file)
        
        # ç®€å•çš„æ‰“æ“‚å°é€»è¾‘ï¼šæˆåŠŸ(1) > å¤±è´¥(0) > æœªçŸ¥(-1)
        if status > best_status:
            best_status = status
            best_root = r_dir
            best_result_str = _read_result_value(res_file)
        elif status == best_status and status == 1:
            # å¦‚æœéƒ½æ˜¯æˆåŠŸï¼Œæ•°å€¼å¤§çš„ä¼˜å…ˆ
            try:
                curr_val = float(_read_result_value(res_file))
                best_val = float(best_result_str)
                if curr_val > best_val:
                    best_root = r_dir
                    best_result_str = str(curr_val)
            except:
                pass
    
    return best_status, best_root, best_result_str

def calculate_global_stats(root_dir, merge_dirs=None):
    """
    è®¡ç®—å…¨å±€ç»Ÿè®¡ä¿¡æ¯ï¼ˆéå†æ‰€æœ‰Domainï¼‰ã€‚
    ç”¨äºåœ¨ Domain é€‰æ‹©ç•Œé¢å±•ç¤ºã€‚
    """
    domains = get_domains(root_dir)
    total_tasks = 0
    total_success = 0
    
    for domain in domains:
        domain_path = os.path.join(root_dir, domain)
        tasks = [t for t in os.listdir(domain_path) if os.path.isdir(os.path.join(domain_path, t))]
        
        for task in tasks:
            total_tasks += 1
            status, _, score = _get_best_task_result(root_dir, domain, task, merge_dirs)
            if status == 1:
                total_success += float(score)
                
    if total_tasks == 0:
        return "### ğŸ“Š å…¨å±€ç»Ÿè®¡: æš‚æ— ä»»åŠ¡æ•°æ®"
    
    success_rate = (total_success / total_tasks) * 100
    stats_text = f"### ğŸŒ å…¨å±€ä»»åŠ¡æˆåŠŸç‡: {success_rate:.2f}% ({total_success}/{total_tasks})"
    if merge_dirs:
        stats_text += f" <span style='font-size:0.8em; color:gray'>(åˆå¹¶æ¨¡å¼å·²å¼€å¯, å…±åˆå¹¶ {len(merge_dirs)+1} ä¸ªè·¯å¾„)</span>"
    return stats_text


def get_tasks_merged(root_dir, domain, compare_dir=None, merge_dirs=None):
    """
    è·å–æŒ‡å®šdomainä¸‹çš„æ‰€æœ‰taskï¼Œæ”¯æŒåˆå¹¶æ¨¡å¼å’Œå¯¹æ¯”æ¨¡å¼ã€‚
    
    é€»è¾‘ï¼š
    1. ä»¥ root_dir (Base) ä¸­çš„ä»»åŠ¡åˆ—è¡¨ä¸ºåŸºå‡†ã€‚
    2. å¦‚æœå¯ç”¨ merge_dirsï¼Œåˆ™åœ¨ Base å’Œæ‰€æœ‰ Merge è·¯å¾„ä¸­å¯»æ‰¾è¯¥ä»»åŠ¡çš„æœ€ä¼˜ç»“æœã€‚
    3. ç»Ÿè®¡åˆå¹¶åçš„æˆåŠŸç‡ã€‚
    4. å¦‚æœå¯ç”¨ compare_dirï¼Œå°† (æ­¥éª¤2ä¸­çš„æœ€ä¼˜ç»“æœ) ä¸ Compare è·¯å¾„çš„ç»“æœè¿›è¡Œå¯¹æ¯”ã€‚
    
    è¿”å›: 
    - task_name_list
    - success_list (æ˜¾ç¤ºçš„æ–‡å­—)
    - css_class_list (æ ·å¼)
    - task_source_map (å­—å…¸: {task_name: best_path_root}) -> ç”¨äºç‚¹å‡»ä»»åŠ¡æ—¶çŸ¥é“å»å“ªé‡ŒåŠ è½½
    - stats_text (ç»Ÿè®¡ä¿¡æ¯çš„Markdownæ–‡æœ¬)
    """
    domain_path = os.path.join(root_dir, domain)
    if not os.path.isdir(domain_path):
        return [], [], [], {}, ""
    
    # ä»¥å½“å‰å¯åŠ¨è·¯å¾„çš„ä»»åŠ¡ä¸ºåŸºå‡†
    task_name_list = [t for t in os.listdir(domain_path) if os.path.isdir(os.path.join(domain_path, t))]
    task_name_list.sort() # æ’åºä¸€ä¸‹æ¯”è¾ƒå¥½çœ‹

    success_list = []
    css_class_list = []
    task_source_map = {} # è®°å½•æ¯ä¸ªä»»åŠ¡åº”è¯¥ä»å“ªä¸ªæ ¹ç›®å½•åŠ è½½
    
    # ç»Ÿè®¡å˜é‡
    total_tasks = 0
    merged_success_count = 0

    # å‡†å¤‡å¾…æ£€æŸ¥çš„è·¯å¾„åˆ—è¡¨: [Base, Merge1, Merge2, ...]
    candidate_roots = [root_dir]
    if merge_dirs:
        candidate_roots.extend(merge_dirs)

    for task_name in task_name_list:
        total_tasks += 1
        
        # --- 1. åˆå¹¶é€»è¾‘ï¼šå¯»æ‰¾æœ€ä¼˜ç»“æœ ---
        best_status = -2 # åˆå§‹åŒ–ä¸€ä¸ªå¾ˆä½çš„çŠ¶æ€
        best_root = root_dir
        best_result_str = "0.0"
        
        # éå†æ‰€æœ‰å€™é€‰è·¯å¾„ï¼Œæ‰¾åˆ†æœ€é«˜çš„
        for r_dir in candidate_roots:
            # æ£€æŸ¥è¯¥è·¯å¾„ä¸‹æ˜¯å¦å­˜åœ¨æ­¤ä»»åŠ¡
            current_task_path = Path(r_dir) / domain / task_name
            if not current_task_path.exists():
                continue
            
            res_file = current_task_path / "result.txt"
            status = _get_result_status(res_file)
            
            # ç®€å•çš„æ‰“æ“‚å°é€»è¾‘ï¼šæˆåŠŸ(1) > å¤±è´¥(0) > æœªçŸ¥(-1)
            # å¦‚æœçŠ¶æ€æ›´å¥½ï¼Œæˆ–è€…çŠ¶æ€ä¸€æ ·ä½†ä¹‹å‰çš„æ˜¯é»˜è®¤è·¯å¾„è€Œç°åœ¨æ˜¯æ–°è·¯å¾„(å¯é€‰)ï¼Œåˆ™æ›´æ–°
            if status > best_status:
                best_status = status
                best_root = r_dir
                best_result_str = _read_result_value(res_file)
            elif status == best_status and status == 1:
                # å¦‚æœéƒ½æ˜¯æˆåŠŸï¼Œæ•°å€¼å¤§çš„ä¼˜å…ˆ (ä¾‹å¦‚ 0.8 vs 1.0)
                try:
                    curr_val = float(_read_result_value(res_file))
                    best_val = float(best_result_str)
                    if curr_val > best_val:
                        best_root = r_dir
                        best_result_str = str(curr_val)
                except:
                    pass

        # è®°å½•è¿™ä¸ªä»»åŠ¡çš„æœ€ä¼˜æºè·¯å¾„
        task_source_map[task_name] = best_root
        
        # ç»Ÿè®¡æˆåŠŸæ•°
        if best_status == 1:
            merged_success_count += float(best_result_str)

        # ç”Ÿæˆæ˜¾ç¤ºçš„æ–‡å­—
        display_text = "æœªçŸ¥"
        if best_status == 1:
            display_text = f'âœ…æˆåŠŸ {best_result_str}âœ…'
        elif best_status == 0:
            display_text = 'âŒå¤±è´¥ 0.0âŒ'
        
        # å¦‚æœæœ€ä¼˜è§£æ¥è‡ªåˆå¹¶è·¯å¾„ï¼Œå¯ä»¥åœ¨æ–‡å­—ä¸Šåšä¸ªæ ‡è®°(å¯é€‰)ï¼Œè¿™é‡Œæš‚ä¸åŠ ï¼Œä¿æŒç®€æ´
        success_list.append(display_text)

        # --- 2. å¯¹æ¯”é€»è¾‘ ---
        # ä½¿ç”¨ "æœ€ä¼˜ç»“æœ(best_status)" å»å’Œ "å¯¹æ¯”è·¯å¾„ç»“æœ" æ¯”è¾ƒ
        current_css_class = ""
        
        # æ£€æŸ¥ Search æ ‡è®° (ä¼˜å…ˆæ£€æŸ¥æœ€ä¼˜è·¯å¾„ä¸‹çš„ search.txt)
        search_flag_file = Path(best_root) / domain / task_name / "search.txt"
        is_search = search_flag_file.exists() and int(search_flag_file.read_text().strip()) == 1
        code_flag_file = Path(best_root) / domain / task_name / "code.txt"
        is_code = code_flag_file.exists() and int(code_flag_file.read_text().strip()) == 1
        
        if compare_dir and os.path.isdir(compare_dir):
            compare_result_file = Path(compare_dir) / domain / task_name / "result.txt"
            compare_status = _get_result_status(compare_result_file)
            
            # åº”ç”¨é¢œè‰²è§„åˆ™
            if best_status == 1 and compare_status == 0:
                current_css_class = "compare-main-win" # èµ¢ (åˆå¹¶åçš„ç»“æœèµ¢)
            elif best_status == 0 and compare_status == 1:
                current_css_class = "compare-comp-win" # è¾“
        
        # å åŠ  Search æ ·å¼
        if is_search and not is_code:
            if best_status == 1:
                current_css_class += " search-and-success"
            else:
                current_css_class += " search-and-failure"

        if is_code and not is_search:
            if best_status == 1:
                current_css_class += " code-and-success"
            else:
                current_css_class += " code-and-failure"
        if is_code and is_search:
            if best_status == 1:
                current_css_class += " code-and-search-success"
            else:
                current_css_class += " code-and-search-failure"
        css_class_list.append(current_css_class)

    # ç”Ÿæˆç»Ÿè®¡æ–‡æœ¬
    if total_tasks > 0:
        success_rate = (merged_success_count / total_tasks) * 100
        stats_text = f"### ğŸ† å½“å‰å±•ç¤ºä»»åŠ¡æˆåŠŸç‡: {success_rate:.2f}% ({merged_success_count}/{total_tasks})"
        if merge_dirs:
            stats_text += " <span style='font-size:0.8em; color:gray'>(å·²åº”ç”¨åˆå¹¶æ¨¡å¼)</span>"
    else:
        stats_text = "### æš‚æ— ä»»åŠ¡æ•°æ®"

    return task_name_list, success_list, css_class_list, task_source_map, stats_text

def load_task_data(root_dir, domain, task):
    """åŠ è½½ä¸€ä¸ªä»»åŠ¡çš„æ‰€æœ‰æ­¥éª¤æ•°æ®å’Œç»“æœ"""
    # æ³¨æ„ï¼šè¿™é‡Œçš„ root_dir åº”è¯¥æ˜¯ task_source_map ä¸­è®°å½•çš„é‚£ä¸ªæœ€ä¼˜è·¯å¾„
    task_path = Path(root_dir) / domain / task
    
    # åŠ è½½ traj.jsonl
    steps = []
    traj_file = task_path / "traj.jsonl"
    if traj_file.exists():
        with open(traj_file, 'r', encoding='utf-8') as f:
            for line in f:
                try:
                    steps.append(json.loads(line.strip()))
                except json.JSONDecodeError:
                    print(f"è­¦å‘Š: åœ¨ {traj_file} ä¸­å‘ç°æ— æ•ˆçš„JSONè¡Œ")

    # åŠ è½½ result.txt
    result_file = task_path / "result.txt"
    result_text = "æœªçŸ¥"
    if result_file.exists():
        result_num = float(result_file.read_text().strip())
        if result_num > 0:
            result_text = f'<span class="success-text">æˆåŠŸ {result_num}</span>'
        elif result_num == 0:
            result_text = '<span class="failure-text">å¤±è´¥ 0.0</span>'
    
    instruction = "æ— æŒ‡ä»¤"
    if steps and "instruction" in steps[0]:
        instruction = steps[0]["instruction"]
        
    return steps, result_text, instruction

def process_code_agent_output(code_agent_output):
    """
    å¤„ç† code_agent_output å­—å…¸ï¼Œæå–å­—æ®µå¹¶åˆå¹¶å†å²è®°å½•ã€‚
    è¿”å›æå–çš„æ•°æ®å’Œå¯è§æ€§æ ‡å¿—ã€‚
    """
    if not code_agent_output or not isinstance(code_agent_output, dict):
        # å¦‚æœæ²¡æœ‰æ•°æ®ï¼Œè¿”å›ç©ºå€¼å’Œ Falseï¼ˆä¸å¯è§ï¼‰
        return "N/A", "N/A", "N/A", "[]", False

    task_instruction = code_agent_output.get("task_instruction", "N/A")
    completion_reason = code_agent_output.get("completion_reason", "N/A")
    summary = code_agent_output.get("summary", "N/A")
    
    exec_history = code_agent_output.get("execution_history", [])
    result_history = code_agent_output.get("execution_result_history", [])

    # ä¸ºäº†é«˜æ•ˆåˆå¹¶ï¼Œåˆ›å»ºä¸€ä¸ªä»¥ step ä¸ºé”®çš„ç»“æœå­—å…¸
    results_map = {item.get('step'): item.get('result') for item in result_history if 'step' in item}

    combined_history = []
    if isinstance(exec_history, list):
        for step_action in exec_history:
            step_num = step_action.get("step")
            # å°† action å’Œ result åˆå¹¶åˆ°åŒä¸€ä¸ªå¯¹è±¡ä¸­
            combined_step = {
                "step": step_num,
                "thoughts": step_action.get("thoughts", "N/A"),
                "action": step_action.get("action", "N/A"),
                "result": results_map.get(step_num, "Result not found for this step.")
            }
            combined_history.append(combined_step)
    
    # å°†åˆå¹¶åçš„åˆ—è¡¨è½¬æ¢ä¸ºæ ¼å¼åŒ–çš„JSONå­—ç¬¦ä¸²
    history_json = json.dumps(combined_history, indent=2)

    # è¿”å›æ‰€æœ‰å¤„ç†å¥½çš„æ•°æ®å’Œ Trueï¼ˆå¯è§ï¼‰
    return task_instruction, completion_reason, summary, history_json, True

def create_gradio_app(root_dir):
    """åˆ›å»ºå¹¶è¿”å›Gradioåº”ç”¨"""
    
    domains = get_domains(root_dir)
    
    # åˆå§‹è®¡ç®—ä¸€æ¬¡å…¨å±€ç»Ÿè®¡
    initial_global_stats = calculate_global_stats(root_dir, merge_dirs=None)

    CUSTOM_CSS = """
        .gr-button-group { display: flex; flex-wrap: wrap; gap: 10px; }
        .gr-button-group > button { flex-grow: 1; }
        .success-text { color: #28a745; font-weight: bold; }
        .failure-text { color: #dc3545; font-weight: bold; }
        #screenshot-container.milestone .gradio-label {
            color: red !important;
            font-weight: bold !important;
        }
        .compare-main-win {
            background: #d4edda !important; 
            border-color: #c3e6cb !important;
        }
        .compare-comp-win {
            background: #f8d7da !important; 
            border-color: #f5c6cb !important;
        }
        .search-and-success::before {
            content: 'search';
            background-image: linear-gradient(to bottom, #2ecc71, #28a745);
            color: white;
            padding: 2px 8px;
            border-radius: 4px;
            font-size: 0.8em;
            font-weight: bold;
            margin-right: 8px;
            display: inline-block;
            vertical-align: middle;
            border: 1px solid #1e7e34;
            box-shadow: 0 1px 1px rgba(0,0,0,0.1);
        }
        .search-and-failure::before {
            content: 'search';
            background-image: linear-gradient(to bottom, #e74c3c, #dc3545);
            color: white;
            padding: 2px 8px;
            border-radius: 4px;
            font-size: 0.8em;
            font-weight: bold;
            margin-right: 8px;
            display: inline-block;
            vertical-align: middle;
            border: 1px solid #b21f2d;
            box-shadow: 0 1px 1px rgba(0,0,0,0.1);
        }
        .code-and-success::before {
            content: 'code';
            background-image: linear-gradient(to bottom, #2ecc71, #28a745);
            color: white;
            padding: 2px 8px;
            border-radius: 4px;
            font-size: 0.8em;
            font-weight: bold;
            margin-right: 8px;
            display: inline-block;
            vertical-align: middle;
            border: 1px solid #1e7e34;
            box-shadow: 0 1px 1px rgba(0,0,0,0.1);
        }
        .code-and-failure::before {
            content: 'code';
            background-image: linear-gradient(to bottom, #e74c3c, #dc3545);
            color: white;
            padding: 2px 8px;
            border-radius: 4px;
            font-size: 0.8em;
            font-weight: bold;
            margin-right: 8px;
            display: inline-block;
            vertical-align: middle;
            border: 1px solid #b21f2d;
            box-shadow: 0 1px 1px rgba(0,0,0,0.1);
        }
        .code-and-search-success::before {
            content: 'code & search';
            background-image: linear-gradient(to bottom, #2ecc71, #28a745);
            color: white;
            padding: 2px 8px;
            border-radius: 4px;
            font-size: 0.8em;
            font-weight: bold;
            margin-right: 8px;
            display: inline-block;
            vertical-align: middle;
            border: 1px solid #1e7e34;
            box-shadow: 0 1px 1px rgba(0,0,0,0.1);
        }
        .code-and-search-failure::before {
            content: 'code & search';
            background-image: linear-gradient(to bottom, #e74c3c, #dc3545);
            color: white;
            padding: 2px 8px;
            border-radius: 4px;
            font-size: 0.8em;
            font-weight: bold;
            margin-right: 8px;
            display: inline-block;
            vertical-align: middle;
            border: 1px solid #b21f2d;
            box-shadow: 0 1px 1px rgba(0,0,0,0.1);
        }
        .stats-header h3 {
            font-size: 1.5rem !important;
            color: #333;
            margin-bottom: 0.5rem;
        }
    """

    with gr.Blocks(theme=gr.themes.Soft(), css=CUSTOM_CSS) as app:
        # --- çŠ¶æ€å­˜å‚¨ ---
        state_root_dir = gr.State(root_dir)
        state_compare_root_dir = gr.State(None)
        state_merge_paths = gr.State([])
        
        state_selected_domain = gr.State()
        state_selected_task = gr.State()
        state_task_source_map = gr.State({}) 
        state_current_viewing_root = gr.State(root_dir)
        
        state_steps_data = gr.State()
        state_current_step_index = gr.State(0)

        # --- è§†å›¾1: Domainé€‰æ‹© ---
        with gr.Column(visible=True) as domain_view:
            gr.Markdown(f"# ä»»åŠ¡è½¨è¿¹æµè§ˆå™¨({os.path.basename(root_dir)})\n")
            
            # 1. Domain ç•Œé¢æ˜¾ç¤ºå…¨å±€ç»Ÿè®¡
            global_stats_display = gr.Markdown(initial_global_stats, elem_classes="stats-header")

            with gr.Accordion("ğŸ› ï¸ é«˜çº§è®¾ç½® (å¯¹æ¯” & åˆå¹¶)", open=False):
                with gr.Tab("ğŸ“Š å¯¹æ¯”æ¨¡å¼"):
                    compare_path_input = gr.Textbox(
                        label="è¾“å…¥å¯¹æ¯”ç»“æœè·¯å¾„", 
                        placeholder="/path/to/another/result",
                        info="è¾“å…¥å¦ä¸€ä¸ªå®éªŒç»“æœçš„æ ¹ç›®å½•ï¼Œç„¶åç‚¹å‡»å¼€å¯å¯¹æ¯”ã€‚"
                    )
                    compare_toggle_btn = gr.Button("ğŸš€ å¼€å¯/æ›´æ–° å¯¹æ¯”æ¨¡å¼")
                    compare_status_text = gr.Markdown("", visible=False)
                
                with gr.Tab("ğŸ”— åˆå¹¶æ¨¡å¼"):
                    merge_paths_input = gr.Textbox(
                        label="è¾“å…¥åˆå¹¶è·¯å¾„åˆ—è¡¨ (æ¯è¡Œä¸€ä¸ªè·¯å¾„)",
                        placeholder="/path/to/result_A\n/path/to/result_B",
                        lines=3,
                        info="è¾“å…¥å¤šä¸ªè·¯å¾„ï¼Œç³»ç»Ÿå°†è‡ªåŠ¨åˆå¹¶å½“å‰è·¯å¾„ä¸è¿™äº›è·¯å¾„ï¼Œå–æ¯ä¸ªä»»åŠ¡çš„æœ€é«˜åˆ†ç»“æœå±•ç¤ºã€‚"
                    )
                    merge_toggle_btn = gr.Button("ğŸ”— å¼€å¯/æ›´æ–° åˆå¹¶æ¨¡å¼")
                    merge_status_text = gr.Markdown("", visible=False)

            with gr.Row():
                gr.Image(label="æˆåŠŸç‡", value=os.path.join(root_dir, "domain_success_rates.png"), type="filepath", interactive=False)
                gr.Image(label="åŠ¨ä½œä½¿ç”¨ç‡", value=os.path.join(root_dir, "overall_action_usage.png"), type="filepath", interactive=False)
                gr.Image(label="æ­¥é•¿/æˆåŠŸç‡", value=os.path.join(root_dir, "overall_step_distribution.png"), type="filepath", interactive=False)
                gr.Image(label="Tokenä½¿ç”¨ç‡", value=os.path.join(root_dir, "overall_token_usage_stacked.png"), type="filepath", interactive=False)

            gr.Markdown("### é€‰æ‹© Domain:")
            with gr.Group(elem_classes="gr-button-group"):
                domain_buttons = []
                for i in range(MAX_BUTTONS):
                    btn = gr.Button(visible=False)
                    domain_buttons.append(btn)
            
            for i, domain_name in enumerate(domains):
                if i < MAX_BUTTONS:
                    domain_buttons[i].value = domain_name
                    domain_buttons[i].visible = True

        # --- è§†å›¾2: Taské€‰æ‹© ---
        with gr.Column(visible=False) as task_view:
            task_view_title = gr.Markdown("# è¯·é€‰æ‹©ä¸€ä¸ª Task")
            # 2. Task ç•Œé¢æ˜¾ç¤º Domain ç»Ÿè®¡
            domain_stats_display = gr.Markdown("", elem_classes="stats-header")
            
            with gr.Row():
                back_to_domains_btn = gr.Button("â¬…ï¸ è¿”å› Domain é€‰æ‹©")
            with gr.Row():
                domain_action_img = gr.Image(label=f"åŠ¨ä½œä½¿ç”¨ç‡", type="filepath", interactive=False)
                domain_step_img = gr.Image(label=f"æ­¥é•¿/æˆåŠŸç‡", type="filepath", interactive=False)
                domain_token_img = gr.Image(label="Tokenä½¿ç”¨ç‡", type="filepath", interactive=False)

            with gr.Group(elem_classes="gr-button-group"):
                task_buttons = []
                success_buttons = []
                for i in range(MAX_BUTTONS):
                    with gr.Row():
                        btn = gr.Button(visible=False)
                        success = gr.Button(visible=False, interactive=False)
                    success_buttons.append(success)
                    task_buttons.append(btn)

        # --- è§†å›¾3: è½¨è¿¹æŸ¥çœ‹å™¨ ---
        with gr.Column(visible=False) as viewer_view:
            viewer_title = gr.Markdown("# æ­£åœ¨æŸ¥çœ‹ä»»åŠ¡")
            with gr.Row():
                back_to_tasks_btn = gr.Button("â¬…ï¸ è¿”å› Task é€‰æ‹©")
            
            step_counter = gr.Markdown("æ­¥éª¤ 1 / N")
            with gr.Row():
                prev_step_btn = gr.Button("â—€ï¸ ä¸Šä¸€æ­¥")
                next_step_btn = gr.Button("â–¶ï¸ ä¸‹ä¸€æ­¥")

            with gr.Row():
                with gr.Column(scale=4):
                    screenshot_img = gr.Image(
                        label="æ­¥éª¤æˆªå›¾", type="filepath", interactive=False, elem_id="screenshot-container"
                    )                    
                    evaluator_json = gr.Code(label="Evaluator", language="json", interactive=False)
                    
                with gr.Column(scale=2):
                    plan_text = gr.Textbox(label="Plan", lines=8, interactive=False)
                    plan_code_text = gr.Code(label="Plan Code", language="python", interactive=False)
                    reflection_text = gr.Textbox(label="Reflection", lines=5, interactive=False)

                    with gr.Accordion(label="Code Agent Plan Details", open=True, visible=False) as code_agent_accordion:
                        task_instruction_text = gr.Textbox(label="Task Instruction", lines=3, interactive=False)
                        completion_reason_text = gr.Textbox(label="Completion Reason", lines=1, interactive=False)
                        summary_text = gr.Textbox(label="Summary", lines=8, interactive=False)
                        execution_history_json = gr.Code(label="Combined Execution History", language="json", interactive=False)
                    with gr.Accordion(label="Search Agent Tutorials", open=True, visible=False) as search_agent_accordion:
                        tutorial_text = gr.Textbox(label="Tutorials", lines=8, interactive=False)

        # =================================================================
        # å‡½æ•°ä¸äº‹ä»¶å¤„ç†
        # =================================================================

        def toggle_comparison(path):
            if path and os.path.isdir(path):
                status_md = f"âœ… **å¯¹æ¯”æ¨¡å¼å·²å¼€å¯ã€‚** å¯¹æ¯”è·¯å¾„: `{path}`"
                return {
                    state_compare_root_dir: path,
                    compare_status_text: gr.update(value=status_md, visible=True),
                    compare_toggle_btn: gr.update(value="ğŸ”„ æ›´æ–°å¯¹æ¯”è·¯å¾„"),
                }
            else:
                return {
                    state_compare_root_dir: None,
                    compare_status_text: gr.update(value="âŒ è·¯å¾„æ— æ•ˆï¼Œå¯¹æ¯”æ¨¡å¼å…³é—­ã€‚", visible=True),
                    compare_toggle_btn: gr.update(value="ğŸš€ å¼€å¯å¯¹æ¯”æ¨¡å¼"),
                }

        def toggle_merge(text_input, root_dir):
            """å¼€å¯åˆå¹¶æ¨¡å¼æ—¶ï¼Œéœ€è¦ç«‹å³è®¡ç®—ä¸€æ¬¡å…¨å±€ç»Ÿè®¡"""
            paths = [line.strip() for line in text_input.split('\n') if line.strip()]
            valid_paths = [p for p in paths if os.path.isdir(p)]
            
            # è®¡ç®—æ–°çš„å…¨å±€ç»Ÿè®¡
            new_stats = calculate_global_stats(root_dir, merge_dirs=valid_paths)
            
            if valid_paths:
                status_md = f"âœ… **åˆå¹¶æ¨¡å¼å·²å¼€å¯ã€‚** æœ‰æ•ˆè·¯å¾„æ•°: {len(valid_paths)}<br>"
                for p in valid_paths:
                    status_md += f"- `{p}`<br>"
                if len(paths) > len(valid_paths):
                    status_md += f"âš ï¸ å¿½ç•¥äº† {len(paths) - len(valid_paths)} ä¸ªæ— æ•ˆè·¯å¾„ã€‚"
                
                return {
                    state_merge_paths: valid_paths,
                    merge_status_text: gr.update(value=status_md, visible=True),
                    merge_toggle_btn: gr.update(value="ğŸ”„ æ›´æ–°åˆå¹¶åˆ—è¡¨"),
                    global_stats_display: gr.update(value=new_stats) # æ›´æ–°å…¨å±€ç»Ÿè®¡
                }
            else:
                return {
                    state_merge_paths: [],
                    merge_status_text: gr.update(value="âŒ æ— æœ‰æ•ˆè·¯å¾„ï¼Œåˆå¹¶æ¨¡å¼å…³é—­ã€‚", visible=True),
                    merge_toggle_btn: gr.update(value="ğŸ”— å¼€å¯åˆå¹¶æ¨¡å¼"),
                    global_stats_display: gr.update(value=new_stats) # æ›´æ–°å…¨å±€ç»Ÿè®¡
                }

        def select_domain(domain_name, current_root_dir, compare_root_dir, merge_dirs):
            """é€‰æ‹©Domainï¼šè®¡ç®—å¹¶å±•ç¤ºè¯¥Domainçš„ç»Ÿè®¡ä¿¡æ¯"""
            # è·å–ä»»åŠ¡åˆ—è¡¨å’ŒDomainç»Ÿè®¡
            tasks, success_list, css_classes, source_map, domain_stats_text = get_tasks_merged(
                current_root_dir, domain_name, compare_dir=compare_root_dir, merge_dirs=merge_dirs
            )
            
            updates = {
                state_selected_domain: domain_name,
                state_task_source_map: source_map,
                domain_view: gr.update(visible=False),
                task_view: gr.update(visible=True),
                task_view_title: gr.update(value=f"# Domain: {domain_name}\nè¯·é€‰æ‹©ä¸€ä¸ª Taskï¼š"),
                domain_stats_display: gr.update(value=domain_stats_text), # æ›´æ–° Task ç•Œé¢çš„ Domain ç»Ÿè®¡
                domain_action_img: gr.update(value=f"{os.path.join(root_dir, f'action_usage_{domain_name}.png')}"),
                domain_step_img: gr.update(value=f"{os.path.join(root_dir, f'step_distribution_{domain_name}.png')}"),
                domain_token_img: gr.update(value=f"{os.path.join(root_dir, f'token_usage_stacked_{domain_name}.png')}"),
            }
            
            for i in range(MAX_BUTTONS):
                if i < len(tasks):
                    updates[task_buttons[i]] = gr.update(
                        value=tasks[i], visible=True, elem_classes=css_classes[i]
                    )
                    updates[success_buttons[i]] = gr.update(
                        value=success_list[i], visible=True
                    )
                else:
                    updates[task_buttons[i]] = gr.update(visible=False, elem_classes="")
                    updates[success_buttons[i]] = gr.update(visible=False)

            return updates
        
        def select_task(task_name, source_map, selected_domain, base_root_dir):
            target_root = source_map.get(task_name, base_root_dir)
            steps, result, instruction = load_task_data(target_root, selected_domain, task_name)
            
            updates = {
                state_selected_task: task_name,
                state_current_viewing_root: target_root,
                state_steps_data: steps,
                state_current_step_index: 0,
                task_view: gr.update(visible=False),
                viewer_view: gr.update(visible=True),
                viewer_title: gr.update(value=f"## {task_name}: {instruction}\n### æœ€ç»ˆç»“æœ: {result}")
            }
            if not steps:
                updates.update({
                    step_counter: "æ²¡æœ‰å¯æ˜¾ç¤ºçš„æ­¥éª¤ã€‚", screenshot_img: None, plan_text: "æ— æ•°æ®",
                    plan_code_text: "æ— æ•°æ®", reflection_text: "æ— æ•°æ®",
                    prev_step_btn: gr.update(interactive=False), next_step_btn: gr.update(interactive=False),
                })
            else:
                step_updates = _get_step_display_updates(steps, 0, target_root, selected_domain, task_name)
                updates.update(step_updates)
            return updates

        def change_step(index, change, steps, viewing_root, domain, task):
            new_index = index + change
            if not (0 <= new_index < len(steps)):
                return {state_current_step_index: index}
            updates = _get_step_display_updates(steps, new_index, viewing_root, domain, task)
            updates[state_current_step_index] = new_index
            return updates

        def _get_step_display_updates(steps, index, root_dir, domain, task):
            step_data = steps[index]
            response = step_data.get("response", {})
            base_path = Path(root_dir) / domain / task
            filename = step_data.get("screenshot_file", "")
            
            img_path = base_path / filename
            annotated_img_path = base_path / (filename[:-4] + "_draw.png")
            milestone_img_path = base_path / (filename[:-4] + "_milestone.png")
            
            if annotated_img_path.exists():
                img_path = annotated_img_path
            elif milestone_img_path.exists():
                img_path = milestone_img_path
            
            is_milestone = "milestone" in str(img_path)
            new_label = "Milestone!" if is_milestone else "æ­¥éª¤æˆªå›¾"
            new_classes = ["milestone"] if is_milestone else []
            
            updates = {
                step_counter: gr.update(value=f"æ­¥éª¤ {index + 1} / {len(steps)}"),
                screenshot_img: gr.update(value=str(img_path) if img_path.exists() else None, label=new_label, elem_classes=new_classes),
                plan_text: gr.update(value=response.get("plan", "N/A")),
                plan_code_text: gr.update(value=response.get("plan_code", "N/A")),
                reflection_text: gr.update(value=response.get("reflection", "N/A")),
                prev_step_btn: gr.update(interactive=index > 0),
                next_step_btn: gr.update(interactive=index < len(steps) - 1),
            }
            
            code_agent_output = response.get("code_agent_output")
            (task_instruction, completion_reason, summary, history_json, is_code_visible) = process_code_agent_output(code_agent_output)
            is_search_visible, tutorial = (True, response["search_agent_output"]["final_answer"]) if response.get("search_agent_output") else (False, "N/A")
            
            evaluator_path = base_path.parent.parent.parent.parent / "evaluation_examples" / "osworld" / "examples" / domain / f"{task}.json"
            if not evaluator_path.exists():
                 evaluator_path = base_path.parent.parent.parent.parent / "evaluation_examples" / "waa" / "examples" / domain / f"{task}.json"

            if evaluator_path.exists():
                try:
                    evaluator_data = json.load(open(evaluator_path, "r", encoding="utf-8"))["evaluator"]
                    if "postconfig" in evaluator_data: del evaluator_data["postconfig"]
                    updates[evaluator_json] = gr.update(value=json.dumps(evaluator_data, indent=2))
                except:
                    updates[evaluator_json] = gr.update(value="æ— æ³•åŠ è½½ Evaluator æ–‡ä»¶")
            else:
                print("[Evaluator]: Evaluator not exists!")
            updates.update({
                code_agent_accordion: gr.update(visible=is_code_visible),
                search_agent_accordion: gr.update(visible=is_search_visible),
                task_instruction_text: gr.update(value=task_instruction),
                completion_reason_text: gr.update(value=completion_reason),
                summary_text: gr.update(value=summary),
                execution_history_json: gr.update(value=history_json),
                tutorial_text: gr.update(value=tutorial),
            })
            return updates

        def back_to_domains_fn(root_dir, merge_dirs):
            """è¿”å›Domainåˆ—è¡¨æ—¶ï¼Œé‡æ–°è®¡ç®—/åˆ·æ–°å…¨å±€ç»Ÿè®¡"""
            stats = calculate_global_stats(root_dir, merge_dirs)
            return {
                domain_view: gr.update(visible=True), 
                task_view: gr.update(visible=False),
                global_stats_display: gr.update(value=stats) # åˆ·æ–°å…¨å±€ç»Ÿè®¡
            }

        def back_to_tasks_fn(selected_domain, stats_text):
            return {
                task_view: gr.update(visible=True), 
                viewer_view: gr.update(visible=False), 
                task_view_title: gr.update(value=f"# Domain: {selected_domain}\nè¯·é€‰æ‹©ä¸€ä¸ª Taskï¼š"),
                domain_stats_display: gr.update(value=stats_text) # ä¿æŒ Domain ç»Ÿè®¡
            }

        # --- ç»‘å®šäº‹ä»¶ ---
        
        compare_toggle_btn.click(
            fn=toggle_comparison,
            inputs=[compare_path_input],
            outputs=[state_compare_root_dir, compare_status_text, compare_toggle_btn]
        )

        merge_toggle_btn.click(
            fn=toggle_merge,
            inputs=[merge_paths_input, state_root_dir],
            outputs=[state_merge_paths, merge_status_text, merge_toggle_btn, global_stats_display]
        )

        domain_click_outputs = [
            state_selected_domain, state_task_source_map, 
            domain_view, task_view, task_view_title, domain_stats_display, 
            domain_action_img, domain_step_img, domain_token_img
        ] + task_buttons + success_buttons
        
        for btn in domain_buttons:
            btn.click(
                fn=select_domain,
                inputs=[btn, state_root_dir, state_compare_root_dir, state_merge_paths],
                outputs=domain_click_outputs
            )
        
        task_select_outputs = [
            state_selected_task, state_current_viewing_root, state_steps_data, state_current_step_index, 
            task_view, viewer_view, viewer_title,
            step_counter, screenshot_img, plan_text, plan_code_text, reflection_text, prev_step_btn, next_step_btn,
            code_agent_accordion, search_agent_accordion, task_instruction_text, completion_reason_text, summary_text, execution_history_json, tutorial_text, evaluator_json
        ]
        for btn in task_buttons: 
            btn.click(
                fn=select_task, 
                inputs=[btn, state_task_source_map, state_selected_domain, state_root_dir], 
                outputs=task_select_outputs
            )
        
        step_change_outputs = [
            state_current_step_index, step_counter, screenshot_img, plan_text, plan_code_text, reflection_text,
            prev_step_btn, next_step_btn,
            code_agent_accordion, search_agent_accordion, task_instruction_text, completion_reason_text, summary_text, execution_history_json, tutorial_text, evaluator_json
        ]
        prev_step_btn.click(
            fn=change_step, 
            inputs=[state_current_step_index, gr.State(-1), state_steps_data, state_current_viewing_root, state_selected_domain, state_selected_task], 
            outputs=step_change_outputs
        )

        next_step_btn.click(
            fn=change_step, 
            inputs=[state_current_step_index, gr.State(1), state_steps_data, state_current_viewing_root, state_selected_domain, state_selected_task], 
            outputs=step_change_outputs
        )
        
        back_to_domains_btn.click(
            fn=back_to_domains_fn, 
            inputs=[state_root_dir, state_merge_paths], 
            outputs=[domain_view, task_view, global_stats_display]
        )
        
        back_to_tasks_btn.click(
            fn=back_to_tasks_fn, 
            inputs=[state_selected_domain, domain_stats_display], 
            outputs=[task_view, viewer_view, task_view_title, domain_stats_display]
        )

    return app

# Helper function to safely extract action from plan_code string
def extract_action_from_plan(plan_code):
    """
    Extracts the action name from a plan_code string like 'agent.click(...)'.
    It looks for the string between the first '.' and the first '('.
    
    Args:
        plan_code (str): The plan code string.
        
    Returns:
        str or None: The extracted action name, or None if the format is unexpected.
    """
    if not isinstance(plan_code, str):
        return None
    
    # Use regex for a more robust extraction
    # This pattern finds a word (alphanumeric + underscore) that is preceded by a '.'
    # and followed by a '('.
    match = re.search(r'\.(.*?)\(', plan_code)
    if match:
        return match.group(1)
    
    # Fallback for simple cases if regex fails, though less robust
    try:
        dot_index = plan_code.find('.')
        paren_index = plan_code.find('(')
        if 0 <= dot_index < paren_index:
            return plan_code[dot_index + 1:paren_index]
    except Exception:
        pass # Ignore errors in string manipulation
        
    return None

# ==============================================================================
# æ–°å¢çš„ç»˜å›¾è¾…åŠ©å‡½æ•°
# ==============================================================================
from matplotlib.ticker import MaxNLocator
def plot_step_histogram(success_steps, failure_steps, title, save_path):
    """
    ä¸ºæˆåŠŸå’Œå¤±è´¥çš„ä»»åŠ¡åˆ›å»ºæ­¥é•¿åˆ†å¸ƒçš„ä¸Šä¸‹å¯¹æ¯”ç›´æ–¹å›¾ã€‚
    (æ–°ç‰ˆï¼šä¸¤ä¸ªå­å›¾å‡ä¸ºæ­£å‘ï¼Œä¸”ä¸Šå­å›¾ä¹Ÿæ˜¾ç¤ºXè½´åˆ»åº¦)

    Args:
        success_steps (list): æˆåŠŸä»»åŠ¡çš„æ­¥é•¿åˆ—è¡¨ã€‚
        failure_steps (list): å¤±è´¥ä»»åŠ¡çš„æ­¥é•¿åˆ—è¡¨ã€‚
        title (str): å›¾è¡¨çš„æ€»æ ‡é¢˜ã€‚
        save_path (str): å›¾ç‰‡ä¿å­˜è·¯å¾„ã€‚
    """
    if not success_steps and not failure_steps:
        print(f"Skipping plot for '{title}' as there is no step data.")
        return

    # åˆ›å»ºä¸¤ä¸ªå‚ç›´æ’åˆ—çš„å­å›¾ï¼Œå¹¶å…±äº«Xè½´
    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10), sharex=True)
    fig.suptitle(title, fontsize=16)

    # è®¡ç®—åˆé€‚çš„binsèŒƒå›´ï¼Œç¡®ä¿è¦†ç›–æ‰€æœ‰æ­¥æ•°
    all_steps = success_steps + failure_steps
    max_step = max(all_steps) if all_steps else 1
    # binsä»1åˆ°max_step+2ï¼Œç¡®ä¿æ¯ä¸ªæ•´æ•°æ­¥é•¿éƒ½æœ‰ç‹¬ç«‹çš„æ¡æŸ±
    bins = range(1, max_step + 3) 

    # --- ä¸Šå­å›¾: æˆåŠŸçš„ä»»åŠ¡ ---
    ax1.hist(success_steps, bins=bins, color='mediumseagreen', alpha=0.8, rwidth=0.8, label='Success')
    ax1.set_title('Successful Tasks')
    ax1.set_ylabel('Number of Tasks')
    
    ax1.tick_params(axis='x', labelbottom=True)
    
    # ç¡®ä¿Yè½´åˆ»åº¦ä¸ºæ•´æ•°
    ax1.yaxis.set_major_locator(MaxNLocator(integer=True))
    ax1.grid(axis='y', linestyle='--', alpha=0.6)
    # ä¸ºä¸Šå­å›¾ä¹Ÿæ·»åŠ Yè½´çš„0çº¿ï¼Œä½¿å…¶çœ‹èµ·æ¥æ›´å®Œæ•´
    ax1.axhline(0, color='black', linewidth=0.8)


    # --- ä¸‹å­å›¾: å¤±è´¥çš„ä»»åŠ¡ ---
    ax2.hist(failure_steps, bins=bins, color='tomato', alpha=0.8, rwidth=0.8, label='Failure')
    ax2.set_title('Failed Tasks')
    ax2.set_xlabel('Number of Steps')
    ax2.set_ylabel('Number of Tasks')
    # ç¡®ä¿Yè½´åˆ»åº¦ä¸ºæ•´æ•°
    ax2.yaxis.set_major_locator(MaxNLocator(integer=True))
    ax2.grid(axis='y', linestyle='--', alpha=0.6)
    # ä¸ºä¸‹å­å›¾ä¹Ÿæ·»åŠ Yè½´çš„0çº¿
    ax2.axhline(0, color='black', linewidth=0.8)

    # è°ƒæ•´å­å›¾ä¹‹é—´çš„é—´è·
    fig.tight_layout(rect=[0, 0.03, 1, 0.95], h_pad=3) # å¢åŠ å‚ç›´é—´è· h_pad

    plt.savefig(save_path)
    plt.close(fig)
    print(f"Saved step distribution plot to {save_path}")


def parse_reflection_type(reflection: str):
    if reflection == "":
        return "On Track"
    if "gui operation error" in reflection.lower():
        return "GUI Operation Error"
    elif "lack of tutorial" in reflection.lower():
        return "Lack of Tutorial"
    elif "code error" in reflection.lower():
        return "Code Error"
    else:
        return "On Track"
    
def plot_token_usage_stacked(stats_data, title, save_path):
    """
    Generates and saves a stacked bar chart for prompt and completion token usage.
    
    Updates:
    1. Logic: Zero-padding is used. Averages are based on the global number of tasks.
       (Even if an agent is not used in a task, it counts as 0 usage).
    2. Export: Saves detailed statistics (Mean, Std, Max, Min) to a JSON file.
    3. Error Bars: Represents Standard Deviation (Mean +/- Std), clipped at 0.
    """
    if not stats_data:
        print(f"Skipping stacked token plot for '{title}' due to no data.")
        return

    try:
        # --- 1. ç¡®å®šå…¨å±€ä»»åŠ¡æ€»æ•° (num_tasks) ---
        # éå†æ‰€æœ‰ Agent çš„æ‰€æœ‰è®°å½•ï¼Œæ‰¾åˆ°æœ€å¤§çš„åˆ—è¡¨é•¿åº¦ä½œä¸ºä»»åŠ¡æ€»æ•°
        num_tasks = 0
        for agent_data in stats_data.values():
            p_len = len(agent_data.get('prompt', []))
            c_len = len(agent_data.get('completion', []))
            num_tasks = max(num_tasks, p_len, c_len)
        
        if num_tasks == 0:
            print(f"Skipping stacked token plot for '{title}' as num_tasks is zero.")
            return

        # å‡†å¤‡æ•°æ®å®¹å™¨
        agents = sorted(stats_data.keys())
        
        # ç”¨äºç»˜å›¾çš„åˆ—è¡¨
        plot_prompt_avgs = []
        plot_completion_avgs = []
        plot_total_stds = [] # è¿™é‡Œå­˜çš„æ˜¯ Total Token (Prompt+Completion) çš„æ ‡å‡†å·®
        
        # ç”¨äº JSON å¯¼å‡ºçš„å­—å…¸
        export_stats = {
            "meta": {
                "title": title,
                "total_tasks": num_tasks,
                "calculation_method": "Global Average (Zero-padded for missing tasks)"
            },
            "agents": {}
        }

        # ç”¨äºè®¡ç®— "Total" (ç³»ç»Ÿçº§) çš„ç´¯åŠ å™¨
        global_prompt_matrix = np.zeros((len(agents), num_tasks))
        global_completion_matrix = np.zeros((len(agents), num_tasks))

        # --- 2. å¤„ç†æ¯ä¸ª Agent çš„æ•°æ® ---
        for idx, agent in enumerate(agents):
            raw_prompts = stats_data[agent].get('prompt', [])
            raw_completions = stats_data[agent].get('completion', [])

            # A. é›¶å¡«å…… (Zero-Padding)
            # å°†æ•°æ®è¡¥é½åˆ° num_tasks é•¿åº¦
            padded_prompts = np.pad(raw_prompts, (0, num_tasks - len(raw_prompts)), 'constant')
            padded_completions = np.pad(raw_completions, (0, num_tasks - len(raw_completions)), 'constant')
            
            # å­˜å…¥çŸ©é˜µä»¥ä¾¿åç»­è®¡ç®— Total
            global_prompt_matrix[idx] = padded_prompts
            global_completion_matrix[idx] = padded_completions

            # B. è®¡ç®—ç»Ÿè®¡é‡
            # å•ä¸ªä»»åŠ¡çš„æ€»æ¶ˆè€— = Prompt + Completion
            agent_task_totals = padded_prompts + padded_completions
            
            p_avg = np.mean(padded_prompts)
            c_avg = np.mean(padded_completions)
            total_avg = np.mean(agent_task_totals)
            total_std = np.std(agent_task_totals)
            total_max = np.max(agent_task_totals)
            total_min = np.min(agent_task_totals)

            # å­˜å…¥ç»˜å›¾åˆ—è¡¨
            plot_prompt_avgs.append(p_avg)
            plot_completion_avgs.append(c_avg)
            plot_total_stds.append(total_std)

            # å­˜å…¥å¯¼å‡ºå­—å…¸
            export_stats["agents"][agent] = {
                "prompt_avg": float(p_avg),
                "completion_avg": float(c_avg),
                "total_avg": float(total_avg),
                "total_std": float(total_std),
                "total_max": float(total_max),
                "total_min": float(total_min)
            }

        # --- 3. è®¡ç®— "Total" (æ‰€æœ‰ Agent åŠ å’Œ) çš„æ•°æ® ---
        # å°†çŸ©é˜µæ²¿è½´ 0 (Agentç»´åº¦) æ±‚å’Œï¼Œå¾—åˆ°æ¯ä¸ªä»»åŠ¡çš„ç³»ç»Ÿæ€»æ¶ˆè€—
        system_task_prompts = np.sum(global_prompt_matrix, axis=0)
        system_task_completions = np.sum(global_completion_matrix, axis=0)
        system_task_totals = system_task_prompts + system_task_completions

        total_p_avg = np.mean(system_task_prompts)
        total_c_avg = np.mean(system_task_completions)
        total_all_avg = np.mean(system_task_totals)
        total_all_std = np.std(system_task_totals)
        total_all_max = np.max(system_task_totals)
        total_all_min = np.min(system_task_totals)

        # æ·»åŠ åˆ°ç»˜å›¾åˆ—è¡¨
        agents.append('Total')
        plot_prompt_avgs.append(total_p_avg)
        plot_completion_avgs.append(total_c_avg)
        plot_total_stds.append(total_all_std)

        # æ·»åŠ åˆ°å¯¼å‡ºå­—å…¸
        export_stats["agents"]["Total"] = {
            "prompt_avg": float(total_p_avg),
            "completion_avg": float(total_c_avg),
            "total_avg": float(total_all_avg),
            "total_std": float(total_all_std),
            "total_max": float(total_all_max),
            "total_min": float(total_all_min)
        }

        # --- 4. ä¿å­˜ JSON æ–‡ä»¶ ---
        json_path = os.path.splitext(save_path)[0] + '.json'
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(export_stats, f, indent=4)
        print(f"Saved token statistics to {json_path}")

        # --- 5. å¼€å§‹ç»˜å›¾ ---
        plt.figure(figsize=(max(10, len(agents) * 1.5), 8))
        
        bar_width = 0.6
        indices = np.arange(len(agents))
        
        p_avgs_np = np.array(plot_prompt_avgs)
        c_avgs_np = np.array(plot_completion_avgs)
        stds_np = np.array(plot_total_stds)
        total_heights = p_avgs_np + c_avgs_np

        # ç»˜åˆ¶ Prompt æŸ±çŠ¶å›¾
        plt.bar(indices, p_avgs_np, bar_width, label='Prompt Tokens', color='#1f77b4', alpha=0.8)
        # ç»˜åˆ¶ Completion æŸ±çŠ¶å›¾ (å †å )
        plt.bar(indices, c_avgs_np, bar_width, bottom=p_avgs_np, label='Completion Tokens', color='#ff7f0e', alpha=0.8)

        # ç»˜åˆ¶è¯¯å·®æ£’
        # é€»è¾‘ï¼šä¸Šé™æ˜¯ Mean + Stdï¼Œä¸‹é™æ˜¯ Mean - Stdã€‚
        # ä½†ä¸ºäº†ä¸è®©è¯¯å·®æ£’ç”»åˆ°è´Ÿæ•°åŒºåŸŸï¼ˆä¸ç¾è§‚ä¸”æ— ç‰©ç†æ„ä¹‰ï¼‰ï¼Œæˆ‘ä»¬å°†ä¸‹é™è¯¯å·®æˆªæ–­ã€‚
        # lower_error = min(total_height, std) æ„å‘³ç€å¦‚æœ std > meanï¼Œä¸‹é™è¯¯å·®æ£’é•¿åº¦ç­‰äº meanï¼Œæ­£å¥½è§¦åº•åˆ°0ã€‚
        lower_errors = np.minimum(total_heights, stds_np)
        upper_errors = stds_np
        asymmetric_errors = np.array([lower_errors, upper_errors])
        
        plt.errorbar(indices, total_heights, yerr=asymmetric_errors, fmt='none', ecolor='black', capsize=5, elinewidth=1.5, markeredgewidth=1.5)

        plt.ylabel('Average Token Count (Global Average)')
        plt.title(f"{title}\n(Averaged over {num_tasks} tasks, including idle runs)")
        plt.xticks(indices, agents, rotation=45, ha='right')
        plt.grid(axis='y', linestyle='--', alpha=0.7)
        plt.legend()

        # --- 6. æ·»åŠ æ•°å€¼æ ‡ç­¾ ---
        for i in range(len(agents)):
            total_h = total_heights[i]
            prompt_h = p_avgs_np[i]
            completion_h = c_avgs_np[i]

            # æ ¼å¼åŒ–æ˜¾ç¤º
            val_str = f'{total_h/1000:,.1f}k' if total_h >= 1000 else f'{total_h:,.0f}'
            
            # å¦‚æœæ–¹å·®æå¤§ï¼Œæ ‡è®°ä¸€ä¸‹
            if stds_np[i] > total_h:
                val_str += "*" # æ ‡è®°è¡¨ç¤ºé«˜æ³¢åŠ¨

            plt.text(indices[i], total_h + upper_errors[i], val_str, ha='center', va='bottom', fontsize=8, fontweight='bold')

        plt.tight_layout()
        plt.savefig(save_path)
        plt.close()
        print(f"Saved stacked token usage plot to {save_path}")

    except Exception as e:
        print(f"An unexpected error occurred while generating stacked token usage plot for '{title}': {e}")
        import traceback
        traceback.print_exc()


def get_result(target_dir):
    """
    Analyzes experiment results from a target directory, calculates success rates,
    gathers action and token statistics, and generates various plots including Error Analysis.
    """
    if not os.path.exists(target_dir):
        print(f"Error: Target directory '{target_dir}' does not exist.")
        return None

    # --- 0. Load Infeasible Task List (æ–°å¢) ---
    infeasible_path = "evaluation_examples/osworld/test_infeasible.json"
    infeasible_task = {}
    if os.path.exists(infeasible_path):
        try:
            infeasible_task = json.load(open(infeasible_path, "r", encoding="utf-8"))
        except Exception as e:
            print(f"Warning: Failed to load infeasible tasks from {infeasible_path}: {e}")
    else:
        # å¦‚æœæ‰¾ä¸åˆ°æ–‡ä»¶ï¼Œé»˜è®¤æ‰€æœ‰ä»»åŠ¡éƒ½æ˜¯ feasibleï¼Œæˆ–è€…ä½ å¯ä»¥æ ¹æ®éœ€è¦è°ƒæ•´è·¯å¾„
        # print(f"Warning: Infeasible task file not found at {infeasible_path}.")
        pass

    # --- Data Structures for Analysis ---
    all_result = []
    domain_result_raw = {} # ä¿ç•™æ­¤ç»“æ„ä»¥å…¼å®¹æ—§é€»è¾‘
    all_result_for_analysis = {}
    overall_action_counts = Counter()
    domain_action_counts = {}

    # --- æ–°å¢ï¼šç”¨äºè¯¦ç»†ç»Ÿè®¡ (All/Feasible/Infeasible) çš„ç»“æ„ ---
    # ç»“æ„: raw_stats[domain][category] = {'scores': [], 'steps': []}
    raw_stats = {} 
    def init_domain_stats():
        return {
            'all':        {'scores': [], 'steps': []},
            'feasible':   {'scores': [], 'steps': []},
            'infeasible': {'scores': [], 'steps': []}
        }

    # --- Token ç»Ÿè®¡ç»“æ„ ---
    domain_token_stats = {}
    overall_token_stats = {}

    # --- Error/Reflection ç»Ÿè®¡ç»“æ„ ---
    # å®šä¹‰ 4 ç±»æ ‡ç­¾ï¼Œä¿æŒè¡Œåˆ—ä¸€è‡´
    COLUMN_LABELS = ["GUI Operation Error", "Lack of Tutorial", "Code Error", "Other Error", "None"]
    ROW_LABELS = ["GUI Error", "Loop Error", "None"]
    def init_error_stats():
        return {
            # è¿™æ˜¯ä¸€ä¸ª 4x4 çš„è®¡æ•°å™¨ï¼š matrix[Row_Hint][Col_Reflection] = count
            'matrix': {r: {c: 0 for c in COLUMN_LABELS} for r in ROW_LABELS},
            'total_steps': 0
        }

    domain_error_stats = {}
    overall_error_stats = init_error_stats()

    # è¾…åŠ©å‡½æ•°ï¼šè§£æ Reflection Type å½’ä¸€åŒ–ä¸º 4 ç±»
    def parse_reflection_type(ref_str: str):
        if not ref_str or ref_str == "None": return "None"
        if "gui operation error" in ref_str.lower(): return "GUI Operation Error"
        if "lack of tutorial" in ref_str.lower(): return "Lack of Tutorial"
        if "code error" in ref_str.lower(): return "Code Error"
        if "other error" in ref_str.lower(): return "Other Error"
        return "None" # å½’ç±»ä¸º Other

    print("Starting analysis...")
    # --- Data Collection Loop ---
    for domain in os.listdir(target_dir):
        domain_path = os.path.join(target_dir, domain)
        if not os.path.isdir(domain_path): continue

        domain_action_counts[domain] = Counter()
        domain_token_stats[domain] = {}
        domain_error_stats[domain] = init_error_stats()
        
        # åˆå§‹åŒ–æ–°ç»Ÿè®¡å®¹å™¨
        if domain not in raw_stats: raw_stats[domain] = init_domain_stats()
        # è·å–å½“å‰ Domain çš„ infeasible ID åˆ—è¡¨
        domain_infeasible_ids = infeasible_task.get(domain, [])

        for example_id in os.listdir(domain_path):
            example_path = os.path.join(domain_path, example_id)
            if not os.path.isdir(example_path): continue

            if domain not in all_result_for_analysis: all_result_for_analysis[domain] = {}
            if example_id not in all_result_for_analysis[domain]: all_result_for_analysis[domain][example_id] = {}

            # åˆ¤æ–­ä»»åŠ¡ç±»å‹
            is_infeasible = example_id in domain_infeasible_ids
            task_type = 'infeasible' if is_infeasible else 'feasible'

            # --- 1. Process Success/Failure Result ---
            result_file = os.path.join(example_path, "result.txt")
            final_result = 0.0
            if os.path.exists(result_file):
                try:
                    with open(result_file, "r") as f: result_str = f.read().strip()
                    try: result_val = float(result_str)
                    except (ValueError, TypeError): result_val = float(eval(result_str))
                    final_result = result_val
                except Exception as e:
                    print(f"Warning: Could not parse result file {result_file}. Defaulting to 0.0. Error: {e}")
                    final_result = 0.0
                
                # æ—§é€»è¾‘ä¿ç•™
                if domain not in domain_result_raw: domain_result_raw[domain] = []
                domain_result_raw[domain].append(final_result)
                all_result.append(final_result)
                
                all_result_for_analysis[domain][example_id]["score"] = final_result

                # --- 2. Process Trajectory for Action and Step Statistics ---
                traj_file = os.path.join(example_path, "traj.jsonl")
                step_count = 0 # é»˜è®¤ä¸º 0
                if os.path.exists(traj_file):
                    try:
                        with open(traj_file, "r", encoding="utf-8") as f:
                            lines = f.readlines()
                            step_count = len(lines) # è·å–æ­¥æ•°
                            all_result_for_analysis[domain][example_id]["step"] = step_count
                            
                            for line in lines:
                                try:
                                    data = json.loads(line)
                                    plan_code = data.get("response", {}).get("plan_code") or data.get("plan_code")
                                    
                                    # æ¨¡æ‹Ÿ action è·å–
                                    action = "unknown"
                                    if plan_code: action = plan_code.split('(')[0]

                                    if action:
                                        overall_action_counts[action] += 1
                                        domain_action_counts[domain][action] += 1
                                    if "call_search_agent" in action:
                                        with open(os.path.join(example_path, "search.txt"), "w", encoding="utf-8") as f:
                                            f.write("1")
                                    if "call_code_agent" in action:
                                        with open(os.path.join(example_path, "code.txt"), "w", encoding="utf-8") as f:
                                            f.write("1")

                                    # --- ErrorType ç»Ÿè®¡é€»è¾‘ (Updated for Heatmap) ---
                                    reflection_data = data.get("response", {}).get("reflection", {})
                                    error_hint = reflection_data.get("hint", {})
                                    
                                    # 1. ç¡®å®š Ground Truth (Hint) - Row
                                    # ä¼˜å…ˆçº§ï¼šå¦‚æœæœ‰æ˜ç¡®çš„ Trueï¼Œå–ç¬¬ä¸€ä¸ªï¼›å¦‚æœå…¨ Falseï¼Œåˆ™ä¸º None/Other
                                    gui_hint = error_hint.get("gui_operation_error", False)
                                    lack_of_tutorial_hint = error_hint.get("lack_of_tutorial", False)
                                    code_hint = error_hint.get("code_error", False)

                                    row_label = "None"
                                    if gui_hint: row_label = "GUI Error"
                                    elif lack_of_tutorial_hint: row_label = "Loop Error"
                                    
                                    # 2. ç¡®å®š Prediction (Reflection) - Column
                                    raw_ref_type = reflection_data.get("reflection", "None")
                                    col_label = parse_reflection_type(raw_ref_type)

                                    # 3. æ›´æ–°ç»Ÿè®¡
                                    # Domain Level
                                    domain_error_stats[domain]['total_steps'] += 1
                                    domain_error_stats[domain]['matrix'][row_label][col_label] += 1

                                    # Overall Level
                                    overall_error_stats['total_steps'] += 1
                                    overall_error_stats['matrix'][row_label][col_label] += 1

                                except (json.JSONDecodeError, AttributeError): continue
                    except Exception as e:
                        print(f"Warning: Could not read or process trajectory file {traj_file}. Error: {e}")
            
                # --- æ–°å¢ï¼šå¡«å……è¯¦ç»†ç»Ÿè®¡æ•°æ® ---
                # 1. å¡«å…¥å¯¹åº”ç±»å‹ (Feasible æˆ– Infeasible)
                raw_stats[domain][task_type]['scores'].append(final_result)
                raw_stats[domain][task_type]['steps'].append(step_count)
                # 2. å¡«å…¥ All ç±»å‹
                raw_stats[domain]['all']['scores'].append(final_result)
                raw_stats[domain]['all']['steps'].append(step_count)


                # --- 3. Process Token Usage ---
                token_log_file = os.path.join(example_path, "token.jsonl")
                if os.path.exists(token_log_file):
                    task_token_summary = {}
                    try:
                        with open(token_log_file, "r", encoding="utf-8") as f:
                            for line in f:
                                try:
                                    data = json.loads(line.strip())
                                    agent_name = data.get("agent_name")
                                    if not agent_name: continue
                                    if agent_name not in task_token_summary:
                                        task_token_summary[agent_name] = {"completion_tokens": 0, "prompt_tokens": 0, "total_tokens": 0}
                                    task_token_summary[agent_name]["completion_tokens"] += data.get("completion_tokens", 0)
                                    task_token_summary[agent_name]["prompt_tokens"] += data.get("prompt_tokens", 0)
                                    task_token_summary[agent_name]["total_tokens"] += data.get("total_tokens", 0)
                                except (json.JSONDecodeError, AttributeError): continue
                        
                        if task_token_summary:
                            task_token_output_path = os.path.join(example_path, "token.json")
                            with open(task_token_output_path, "w", encoding="utf-8") as f:
                                json.dump(task_token_summary, f, indent=4)
                            
                            for agent, tokens in task_token_summary.items():
                                if agent not in domain_token_stats[domain]:
                                    domain_token_stats[domain][agent] = {'prompt': [], 'completion': []}
                                domain_token_stats[domain][agent]['prompt'].append(tokens['prompt_tokens'])
                                domain_token_stats[domain][agent]['completion'].append(tokens['completion_tokens'])
                                
                                if agent not in overall_token_stats:
                                    overall_token_stats[agent] = {'prompt': [], 'completion': []}
                                overall_token_stats[agent]['prompt'].append(tokens['prompt_tokens'])
                                overall_token_stats[agent]['completion'].append(tokens['completion_tokens'])

                    except Exception as e:
                        print(f"Warning: Could not process token file {token_log_file}. Error: {e}")

    # --- Result Summary and JSON Output ---
    if not all_result:
        print("New experiment or no valid results found.")
        return None

    # --- æ–°å¢ï¼šæ ¼å¼åŒ–æ‰“å°å‡½æ•° ---
    def print_metrics(label, data_dict):
        """
        data_dict ç»“æ„: {'all': {'scores':[], 'steps':[]}, 'feasible': ..., 'infeasible': ...}
        """
        def get_stats(cat):
            scores = data_dict[cat]['scores']
            steps = data_dict[cat]['steps']
            count = len(scores)
            if count == 0: return "N/A", "N/A", 0
            sr = sum(scores) / count * 100
            avg_steps = sum(steps) / count
            return f"{sr:.2f}%", f"{avg_steps:.1f}", count

        sr_all, step_all, cnt_all = get_stats('all')
        sr_fea, step_fea, cnt_fea = get_stats('feasible')
        sr_inf, step_inf, cnt_inf = get_stats('infeasible')

        print(f"{label:<20} | "
              f"ALL: SR={sr_all:<5} Stp={step_all:<4} ({cnt_all}) | "
              f"FEA: SR={sr_fea:<5} Stp={step_fea:<4} ({cnt_fea}) | "
              f"INF: SR={sr_inf:<5} Stp={step_inf:<4} ({cnt_inf})")

    # --- æ‰“å°è¡¨å¤´ ---
    print("\n" + "="*120)
    print(f"{'Domain Analysis':<20} | {'All Tasks':<30} | {'Feasible Tasks':<30} | {'Infeasible Tasks':<30}")
    print(f"{'':<20} | {'SR':<6} {'Step':<5} {'(Num)':<6}      | {'SR':<6} {'Step':<5} {'(Num)':<6}      | {'SR':<6} {'Step':<5} {'(Num)':<6}")
    print("-" * 120)

    # 1. Sub-Domain ç»Ÿè®¡
    domain_success_rate = {} # é‡å»ºæ­¤å­—å…¸ä»¥ä¾› Plot 3 ä½¿ç”¨
    sorted_domains = sorted(raw_stats.keys())
    for domain in sorted_domains:
        print_metrics(domain, raw_stats[domain])
        # é‡å»º domain_success_rate ç”¨äºåç»­ç»˜å›¾
        scores = raw_stats[domain]['all']['scores']
        if scores:
            domain_success_rate[domain] = sum(scores) / len(scores) * 100
    
    print("-" * 120)

    # 2. Father Domain ç»Ÿè®¡
    # åŠ¨æ€åˆ¤æ–­ä½¿ç”¨å“ªå¥—æ˜ å°„
    if "thunderbird" in raw_stats.keys():
        father_domain_mapping = {
            "OS": ["os"],
            "Office": ["libreoffice_calc", "libreoffice_impress", "libreoffice_writer"],
            "Daily": ["chrome", "vlc", "thunderbird"],
            "Professional": ["vscode", "gimp"],
            "Workflow": ["multi_apps"]
        }
    elif "msedge" in raw_stats.keys():
        father_domain_mapping = {
            "Office": ["libreoffice_writer", "libreoffice_calc"],
            "Web Browing": ["msedge", "chrome"],
            "Windows System": ["file_explorer", "settings"],
            "Coding": ["vs_code"],
            "Media & Video": ["vlc"],
            "Windows Utilities": ["microsoft_paint",  "clock", "windows_calc", "notepad"]
        }
    else:
        father_domain_mapping = {
            "SingleApps": ["calendar", "clock", "finder", "mac_system_settings", "notes", "reminders", "safari", "terminal", "vscode"],
            "MultiApps": ["multi_app"]
        }
    if father_domain_mapping:
        for father, children in father_domain_mapping.items():
            father_stats = init_domain_stats()
            has_data = False
            for child in children:
                if child in raw_stats:
                    has_data = True
                    for cat in ['all', 'feasible', 'infeasible']:
                        father_stats[cat]['scores'].extend(raw_stats[child][cat]['scores'])
                        father_stats[cat]['steps'].extend(raw_stats[child][cat]['steps'])
            
            if has_data:
                print_metrics(f"[F] {father}", father_stats)
        print("-" * 120)

    # 3. Overall ç»Ÿè®¡
    overall_stats = init_domain_stats()
    for domain in raw_stats:
        for cat in ['all', 'feasible', 'infeasible']:
            overall_stats[cat]['scores'].extend(raw_stats[domain][cat]['scores'])
            overall_stats[cat]['steps'].extend(raw_stats[domain][cat]['steps'])
    
    print_metrics("OVERALL", overall_stats)
    print("=" * 120)

    # è®¡ç®— overall_rate ä¾› Plot 3 ä½¿ç”¨
    overall_rate = 0.0
    if all_result:
        overall_rate = sum(all_result) / len(all_result) * 100

    json_output_path = os.path.join(target_dir, "all_result_summary.json")
    try:
        with open(json_output_path, "w", encoding="utf-8") as f: json.dump(all_result_for_analysis, f, indent=4)
        print(f"\nAnalysis summary saved to {json_output_path}")
    except Exception as e: print(f"Error saving summary JSON: {e}")

    # --- Plotting Section ---
    print("\nGenerating plots...")

    # Plot 1: Overall Action Usage
    if overall_action_counts:
        try:
            save_path = os.path.join(target_dir, "overall_action_usage.png")
            plt.figure(figsize=(12, 8)); sorted_actions = overall_action_counts.most_common(); actions = [i[0] for i in sorted_actions]; counts = [i[1] for i in sorted_actions]
            bars = plt.barh(actions, counts, color='skyblue'); plt.xlabel('Usage Count'); plt.ylabel('Action Type'); plt.title('Overall Action Usage Frequency'); plt.gca().invert_yaxis()
            if counts: plt.xlim(right=max(counts) * 1.15)
            for bar in bars: xval = bar.get_width(); plt.text(xval + (max(counts) * 0.01), bar.get_y() + bar.get_height() / 2.0, f' {int(xval)} ({int(xval) / sum(counts) * 100:.1f}%)', ha='left', va='center')
            plt.tight_layout(); plt.savefig(save_path); plt.close(); print(f"Saved overall action usage plot to {save_path}")
        except Exception as e: print(f"Error generating overall action usage plot: {e}")

    # Plot 2: Per-Domain Action Usage
    for domain, counts in domain_action_counts.items():
        if not counts: continue
        try:
            save_path = os.path.join(target_dir, f"action_usage_{domain}.png")
            plt.figure(figsize=(10, 6))
            
            sorted_actions = counts.most_common()
            actions = [i[0] for i in sorted_actions]
            action_counts = [i[1] for i in sorted_actions] 
            
            bars = plt.barh(actions, action_counts, color='lightgreen')
            plt.xlabel('Usage Count')
            plt.ylabel('Action Type')
            plt.title(f'Action Usage Frequency in Domain: {domain}')
            plt.gca().invert_yaxis()
            
            if action_counts: 
                plt.xlim(right=max(action_counts) * 1.15)
            
            total_count = sum(action_counts)

            for bar in bars: 
                xval = bar.get_width()
                plt.text(xval + (max(action_counts) * 0.01), 
                         bar.get_y() + bar.get_height() / 2.0, 
                         f' {int(xval)} ({int(xval) / total_count * 100:.1f}%)', 
                         ha='left', va='center')
            
            plt.tight_layout()
            plt.savefig(save_path)
            plt.close()
            print(f"Saved action usage plot for domain '{domain}' to {save_path}")
        except Exception as e: 
            print(f"Error generating action usage plot for domain {domain}: {e}")

    # Plot 3: Success Rate by Domain
    if domain_success_rate:
        try:
            save_path = os.path.join(target_dir, "domain_success_rates.png"); domains_sorted = sorted(domain_success_rate.keys()); rates_sorted = [domain_success_rate[d] for d in domains_sorted]
            plot_labels = domains_sorted + ['Average']; plot_values = rates_sorted + [overall_rate]; colors = ['#87CEEB'] * len(domains_sorted) + ['#FF6347']
            plt.figure(figsize=(max(10, len(plot_labels) * 0.8), 7)); bars = plt.bar(plot_labels, plot_values, color=colors)
            plt.ylabel('Success Rate (%)'); plt.title('Success Rate by Domain and Overall Average'); plt.xticks(rotation=45, ha='right'); plt.ylim(0, 110); plt.grid(axis='y', linestyle='--', alpha=0.7)
            for bar in bars: yval = bar.get_height(); plt.text(bar.get_x() + bar.get_width()/2.0, yval + 1.5, f'{yval:.1f}%', ha='center', va='bottom')
            plt.tight_layout(); plt.savefig(save_path); plt.close(); print(f"Saved success rate plot to {save_path}")
        except Exception as e: print(f"Error generating success rate plot: {e}")


    # --- Plot 4: Step Distribution Histograms ---
    step_stats = {'overall': {'success_steps': [], 'failure_steps': []}}
    for domain, tasks in all_result_for_analysis.items():
        if domain not in step_stats: step_stats[domain] = {'success_steps': [], 'failure_steps': []}
        for task_id, data in tasks.items():
            if data.get('score') is not None and data.get('step') is not None:
                if data['score'] > 0.0: 
                    step_stats[domain]['success_steps'].append(data['step'])
                    step_stats['overall']['success_steps'].append(data['step'])
                else: 
                    step_stats[domain]['failure_steps'].append(data['step'])
                    step_stats['overall']['failure_steps'].append(data['step'])

    for name, data in step_stats.items():
        save_path = os.path.join(target_dir, 'overall_step_distribution.png' if name == 'overall' else f'step_distribution_{name}.png')
        title = f"{'Overall' if name == 'overall' else 'Domain: ' + name} Task Outcome by Number of Steps"
        try:
            # å‡è®¾ plot_step_histogram å­˜åœ¨
            if name == "overall":
                overall_step_stat = {
                    "success_steps": data['success_steps'],
                    "failure_steps": data['failure_steps']
                }
                with open(os.path.join(target_dir, "step_stat.json"), "w", encoding="utf-8") as f:
                    json.dump(overall_step_stat, f, indent=4)
                # print(f"Success Step: {data['success_steps']}, Failure Step: {data['failure_steps']}")
            plot_step_histogram(data['success_steps'], data['failure_steps'], title, save_path)
        except NameError:
            # å¦‚æœå¤–éƒ¨æ²¡æœ‰å®šä¹‰è¯¥å‡½æ•°ï¼Œè·³è¿‡
            pass



    # --- Plot 5: è°ƒç”¨æ–°çš„å †å å›¾å‡½æ•° ---
    print("\nGenerating stacked token usage plots...")
    try:
        # ä¸ºæ¯ä¸ª domain ç”Ÿæˆå›¾è¡¨
        for domain, token_data in domain_token_stats.items():
            plot_token_usage_stacked(
                stats_data=token_data,
                title=f'Average Token Usage (Stacked) per Task in Domain: {domain}',
                save_path=os.path.join(target_dir, f"token_usage_stacked_{domain}.png")
            )
        plot_token_usage_stacked(
            stats_data=overall_token_stats,
            title='Overall Average Token Usage (Stacked) per Task',
            save_path=os.path.join(target_dir, "overall_token_usage_stacked.png")
        )
    except NameError:
        print("Warning: plot_token_usage_stacked function not found. Skipping token plots.")


    # --- Plot 6: Error & Reflection Analysis (Heatmap / Confusion Matrix) ---
    print("\nGenerating error analysis heatmaps...")

    def plot_confusion_heatmap(stats, title_prefix, save_path):
        """
        Generates a 4x4 Heatmap.
        Y-axis (Rows): Ground Truth (Hint)
        X-axis (Cols): Agent Reflection (Predicted)
        """
        if stats['total_steps'] == 0:
            return

        # å‡†å¤‡æ•°æ®çŸ©é˜µ (4x4)
        data_matrix = []
        
        for row_label in ROW_LABELS:
            row_data = []
            for col_label in COLUMN_LABELS:
                row_data.append(stats['matrix'][row_label][col_label])
            data_matrix.append(row_data)
        
        data_np = np.array(data_matrix)

        # å¼€å§‹ç»˜å›¾
        fig, ax = plt.subplots(figsize=(10, 8))
        
        # ä½¿ç”¨ imshow ç»˜åˆ¶çƒ­åŠ›å›¾
        # cmap='OrRd' (æ©™çº¢) æˆ– 'Blues' (è“) éƒ½ä¸é”™
        im = ax.imshow(data_np, cmap='Blues')

        # è®¾ç½®åæ ‡è½´
        ax.set_xticks(np.arange(len(COLUMN_LABELS)))
        ax.set_yticks(np.arange(len(ROW_LABELS)))
        
        # æ ‡ç­¾æ¢è¡Œå¤„ç†ï¼Œé˜²æ­¢é‡å 
        formatted_labels = [l.replace(" ", "\n") for l in COLUMN_LABELS]
        ax.set_xticklabels(formatted_labels, fontsize=10)
        ax.set_yticklabels(ROW_LABELS, fontsize=10)

        # è½´æ ‡é¢˜
        ax.set_xlabel("Agent Reflection (Predicted)", fontsize=12, fontweight='bold')
        ax.set_ylabel("Environment Hint (Ground Truth)", fontsize=12, fontweight='bold')
        
        # å°† X è½´æ ‡ç­¾ç§»åˆ°é¡¶éƒ¨ï¼Œæˆ–è€…ä¿æŒåœ¨åº•éƒ¨ä½†æ—‹è½¬
        plt.setp(ax.get_xticklabels(), rotation=0, ha="center", rotation_mode="anchor")

        # æ ‡é¢˜
        ax.set_title(f"{title_prefix}\nReflection Confusion Matrix", fontsize=14, pad=20)

        # æ·»åŠ é¢œè‰²æ¡
        cbar = ax.figure.colorbar(im, ax=ax)
        cbar.ax.set_ylabel("Count", rotation=-90, va="bottom")

        # åœ¨æ¯ä¸ªæ ¼å­é‡Œå¡«å…¥æ•°å­—
        # é˜ˆå€¼ç”¨äºè‡ªåŠ¨è°ƒæ•´å­—ä½“é¢œè‰²ï¼ˆæ·±è‰²èƒŒæ™¯ç”¨ç™½å­—ï¼Œæµ…è‰²èƒŒæ™¯ç”¨é»‘å­—ï¼‰
        threshold = data_np.max() / 2.
        
        total_count = data_np.sum()

        for i in range(len(ROW_LABELS)): # Row
            for j in range(len(COLUMN_LABELS)): # Col
                count = data_np[i, j]
                # è®¡ç®—è¯¥æ ¼å­çš„ç™¾åˆ†æ¯” (å æ€»æ­¥æ•°çš„æ¯”ä¾‹)
                pct = (count / total_count * 100) if total_count > 0 else 0
                
                text_color = "white" if count > threshold else "black"
                
                # æ˜¾ç¤ºæ ¼å¼ï¼šæ•°é‡ (ç™¾åˆ†æ¯”)
                text_str = f"{count}\n({pct:.1f}%)"
                
                ax.text(j, i, text_str, ha="center", va="center", color=text_color, fontsize=11, fontweight='bold')

        plt.tight_layout()
        plt.savefig(save_path)
        plt.close()
        print(f"Saved error heatmap to {save_path}")

    # Generate Overall Plot
    print(f'Overall Error Stats: {overall_error_stats}')
    plot_confusion_heatmap(overall_error_stats, "Overall", os.path.join(target_dir, "overall_error_analysis_heatmap.png"))

    # Generate Per-Domain Plots
    for domain, stats in domain_error_stats.items():
        plot_confusion_heatmap(stats, f"Domain: {domain}", os.path.join(target_dir, f"error_analysis_heatmap_{domain}.png"))

    print("\nAnalysis complete.")
    return all_result


# ==============================================================================
# ä¸»ç¨‹åºå…¥å£
# ==============================================================================
if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser()

    # environment config
    parser.add_argument("--root_dir", type=str, required=True)
    parser.add_argument("--port", type=int, default=10000)
    args = parser.parse_args()
    
    get_result(args.root_dir)
    gradio_app = create_gradio_app(args.root_dir)
    gradio_app.launch(server_name="0.0.0.0", server_port=args.port, allowed_paths=["/nvme/yangbowen/"])
